{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "We will perform EDA on the dataset of SQL injection available on Kaggle: [dataset_link](https://www.kaggle.com/sajid576/sql-injection-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:42.363029Z",
     "iopub.status.busy": "2021-12-04T03:51:42.362594Z",
     "iopub.status.idle": "2021-12-04T03:51:42.494570Z",
     "shell.execute_reply": "2021-12-04T03:51:42.493624Z",
     "shell.execute_reply.started": "2021-12-04T03:51:42.362982Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sql.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m raw_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m raw_dataset\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32m~\\Pictures\\New folder\\New folder\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(io, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, engine\u001b[38;5;241m=\u001b[39mengine)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32m~\\Pictures\\New folder\\New folder\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1497\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32m~\\Pictures\\New folder\\New folder\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1372\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\Pictures\\New folder\\New folder\\Lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sql.xlsx'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "raw_dataset = pd.read_excel(\"sql.xlsx\")\n",
    "raw_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null/missing values in the dataset.  \n",
    "We have two columns:\n",
    "1. **Query**: SQL query\n",
    "2. **Label**: 1 if injection 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:42.496194Z",
     "iopub.status.busy": "2021-12-04T03:51:42.495937Z",
     "iopub.status.idle": "2021-12-04T03:51:42.511578Z",
     "shell.execute_reply": "2021-12-04T03:51:42.510592Z",
     "shell.execute_reply.started": "2021-12-04T03:51:42.496163Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:42.514250Z",
     "iopub.status.busy": "2021-12-04T03:51:42.513520Z",
     "iopub.status.idle": "2021-12-04T03:51:42.762220Z",
     "shell.execute_reply": "2021-12-04T03:51:42.761398Z",
     "shell.execute_reply.started": "2021-12-04T03:51:42.514204Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_dataset['Label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive class** - 11382  \n",
    "**Negative class** - 19537  \n",
    "Dataset is imbalanced with popsitive to negative class ratio of 37:63 (~ 4:7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[raw_dataset.duplicated(keep=False)].sort_values(by=['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we have found 20 duplicate values.\n",
    "* All values looks like number and not like SQL query.\n",
    "* Let us remove all the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so many white spaces in the queries and multiple special characters.  \n",
    "We can try to train model by removing the special characters.  \n",
    "If this does not work, we can include special characters afterwards.  \n",
    "Let's create two datasets one with and other without special characters.  \n",
    "This all are SQL queries so no need to remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.956840Z",
     "iopub.status.busy": "2021-12-04T03:51:48.955855Z",
     "iopub.status.idle": "2021-12-04T03:51:48.960850Z",
     "shell.execute_reply": "2021-12-04T03:51:48.959886Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.956801Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.962920Z",
     "iopub.status.busy": "2021-12-04T03:51:48.962316Z",
     "iopub.status.idle": "2021-12-04T03:51:48.977718Z",
     "shell.execute_reply": "2021-12-04T03:51:48.976419Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.962872Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_query = raw_dataset['Query'][9780]\n",
    "sample_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.979482Z",
     "iopub.status.busy": "2021-12-04T03:51:48.979145Z",
     "iopub.status.idle": "2021-12-04T03:51:48.989646Z",
     "shell.execute_reply": "2021-12-04T03:51:48.988618Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.979453Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_spaces(input_string):\n",
    "    cleaned = re.sub('\\s{2,}',' ',input_string)\n",
    "    return cleaned.lower().strip()\n",
    "\n",
    "print(remove_spaces(sample_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.991408Z",
     "iopub.status.busy": "2021-12-04T03:51:48.991027Z",
     "iopub.status.idle": "2021-12-04T03:51:49.003149Z",
     "shell.execute_reply": "2021-12-04T03:51:49.002474Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.991363Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_all(input_string):\n",
    "    cleaned = re.sub('[^a-zA-Z0-9\\s]',' ',input_string)\n",
    "    cleaned = re.sub('\\s{2,}',' ',cleaned)\n",
    "    return cleaned.lower().strip()\n",
    "print(clean_all(sample_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.025351Z",
     "iopub.status.busy": "2021-12-04T03:51:49.024979Z",
     "iopub.status.idle": "2021-12-04T03:51:49.236207Z",
     "shell.execute_reply": "2021-12-04T03:51:49.235404Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.025306Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_with_special_characters = raw_dataset.copy()\n",
    "dataset_with_special_characters['Query'] = dataset_with_special_characters['Query'].apply(remove_spaces)\n",
    "dataset_with_special_characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.238339Z",
     "iopub.status.busy": "2021-12-04T03:51:49.237474Z",
     "iopub.status.idle": "2021-12-04T03:51:49.614490Z",
     "shell.execute_reply": "2021-12-04T03:51:49.613606Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.238291Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_all_cleaned = raw_dataset.copy()\n",
    "dataset_all_cleaned['Query'] = dataset_all_cleaned['Query'].apply(clean_all)\n",
    "dataset_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if we have any duplicates after removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_cleaned[dataset_all_cleaned.duplicated(keep=False)].sort_values(by='Query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_cleaned.drop_duplicates(keep='first', inplace=True)\n",
    "dataset_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special Characters Count**  \n",
    "Let's find out if number of special characters in the query has any relation with dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_special_chars = raw_dataset['Query'].map(lambda x:len(re.findall('[^a-zA-Z0-9\\s]',x)))\n",
    "no_of_special_chars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax_sc = plt.subplots(1, 1, figsize=(15,8))\n",
    "sns.histplot(no_of_special_chars[raw_dataset['Label']==1], ax=ax_sc, color='red', label='Malicious', binrange=(0,100) ,alpha=0.4)\n",
    "sns.histplot(no_of_special_chars[raw_dataset['Label']==0], ax=ax_sc, color='green', label='Nomral', binrange=(0,100))\n",
    "ax_sc.set_title(\"Special Character Distribution\")\n",
    "ax_sc.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Here we can see, Number of special characters in a query has almost sane distribution for normal and malicious queries. This is not a good feature to make classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the duplicates and keep the first instance only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_length = dataset_all_cleaned.Query.map(lambda x:len(x.split()))\n",
    "query_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:42.819798Z",
     "iopub.status.busy": "2021-12-04T03:51:42.819594Z",
     "iopub.status.idle": "2021-12-04T03:51:42.825775Z",
     "shell.execute_reply": "2021-12-04T03:51:42.824824Z",
     "shell.execute_reply.started": "2021-12-04T03:51:42.819773Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12,12))\n",
    "sns.histplot(query_length, binrange=(0,50), ax=ax1)\n",
    "sns.histplot(query_length, binrange=(50,75), ax=ax2)\n",
    "sns.histplot(query_length, binrange=(75,100), ax=ax3)\n",
    "sns.histplot(query_length, binrange=(100,600), ax=ax4)\n",
    "plt.suptitle(\"Query Length Distribution\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.748995Z",
     "iopub.status.busy": "2021-12-04T03:51:48.748660Z",
     "iopub.status.idle": "2021-12-04T03:51:48.775415Z",
     "shell.execute_reply": "2021-12-04T03:51:48.774539Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.748954Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"0 words   =\", (query_length==0).value_counts()[True])\n",
    "print(\"1 words   =\", (query_length==1).value_counts()[True])\n",
    "print(\"2 words   =\", (query_length==2).value_counts()[True])\n",
    "print(\"Less than 5 words   =\", (query_length<5).value_counts()[True])\n",
    "print(\"Less than 10 words  =\", (query_length<10).value_counts()[True])\n",
    "print(\"Less than 15 words  =\", (query_length<15).value_counts()[True])\n",
    "print(\"Less than 50 words  =\", (query_length<50).value_counts()[True])\n",
    "print(\"More than 90 words =\", (query_length>90).value_counts()[True])\n",
    "print(\"More than 500 words =\", (query_length>500).value_counts()[True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have one outlier where we do not have any query.\n",
    "We need to find that and remove from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.777456Z",
     "iopub.status.busy": "2021-12-04T03:51:48.776947Z",
     "iopub.status.idle": "2021-12-04T03:51:48.807590Z",
     "shell.execute_reply": "2021-12-04T03:51:48.806939Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.777401Z"
    }
   },
   "outputs": [],
   "source": [
    "for per in range(10,101,10):\n",
    "    print(f'{per} percentile for query length = {np.percentile(query_length, per)}')\n",
    "print('-'*40)\n",
    "for per in range(91,101,1):\n",
    "    print(f'{per} percentile for query length = {np.percentile(query_length, per)}')\n",
    "print('-'*40)\n",
    "for per in np.linspace(99,100,11):\n",
    "    print(f'{per} percentile for query length = {np.percentile(query_length, per)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**  \n",
    "From histogram, percentiles and other observations for query length:\n",
    "1. 90% of the queries are having 19 or less words.\n",
    "2. 95% of the queries are having 26 or less words.\n",
    "3. Less than 1% of the data have more than 43 words.\n",
    "4. Only one outlier query has 542 words.\n",
    "5. Only one outlier query has 0 words (only space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlier Removal**  \n",
    "Let's remove query with length 0(minimum) and 542(maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_cleaned[dataset_all_cleaned['Query'].map(lambda x:len(x.split()))==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_cleaned[dataset_all_cleaned['Query'].map(lambda x:len(x.split()))==542]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.821660Z",
     "iopub.status.busy": "2021-12-04T03:51:48.821329Z",
     "iopub.status.idle": "2021-12-04T03:51:48.832486Z",
     "shell.execute_reply": "2021-12-04T03:51:48.831507Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.821632Z"
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_colwidth', None):\n",
    "long_query=dataset_all_cleaned[dataset_all_cleaned['Query'].map(lambda x:len(x.split()))==542]['Query'].item()\n",
    "print(long_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above two data points are surely noisy and we can remove them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:48.838092Z",
     "iopub.status.busy": "2021-12-04T03:51:48.837709Z",
     "iopub.status.idle": "2021-12-04T03:51:48.860568Z",
     "shell.execute_reply": "2021-12-04T03:51:48.859588Z",
     "shell.execute_reply.started": "2021-12-04T03:51:48.838061Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_all_cleaned.drop([208,19341], inplace=True)\n",
    "dataset_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check query length distribution for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_queries = dataset_all_cleaned[dataset_all_cleaned['Label']==0]\n",
    "malicious_queries = dataset_all_cleaned[dataset_all_cleaned['Label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_queries_length = normal_queries.Query.map(lambda x:len(x.split()))\n",
    "malicious_queries_length = malicious_queries.Query.map(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ((ax21, ax22)) = plt.subplots(1, 2, figsize=(15,6))\n",
    "sns.histplot(normal_queries_length, binrange=(0,50), ax=ax21, color='green', label='Nomral')\n",
    "sns.histplot(normal_queries_length, binrange=(50,150), ax=ax22, color='green', label='Nomral')\n",
    "sns.histplot(malicious_queries_length, binrange=(0,50), ax=ax21, color='red', label='Malicious', alpha=0.4)\n",
    "sns.histplot(malicious_queries_length, binrange=(50,150), ax=ax22, color='red', label='Malicious', alpha=0.4)\n",
    "ax21.set_title(\"Normal vs Malicious Queries\")\n",
    "ax22.set_title(\"Normal vs Malicious Queries\")\n",
    "ax21.legend()\n",
    "ax22.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Query length for both classes are having almost same distribution so it won't add any value classifying dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model and feature selection  \n",
    "Here our goal is to find out the words or series of words which are common in attacks.  \n",
    "We can either use TFIDF or simple tokenization which will find out the pattern in n-gram features to identify the attack.  \n",
    "**NOTE**: We will first try with all cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset_all_cleaned.df', 'wb') as file:\n",
    "  pickle.dump(dataset_all_cleaned, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.622978Z",
     "iopub.status.busy": "2021-12-04T03:51:49.622430Z",
     "iopub.status.idle": "2021-12-04T03:51:49.633968Z",
     "shell.execute_reply": "2021-12-04T03:51:49.632932Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.622942Z"
    }
   },
   "outputs": [],
   "source": [
    "X = dataset_all_cleaned['Query']\n",
    "y = dataset_all_cleaned['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.635540Z",
     "iopub.status.busy": "2021-12-04T03:51:49.635189Z",
     "iopub.status.idle": "2021-12-04T03:51:49.834158Z",
     "shell.execute_reply": "2021-12-04T03:51:49.833173Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.635504Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.836167Z",
     "iopub.status.busy": "2021-12-04T03:51:49.835834Z",
     "iopub.status.idle": "2021-12-04T03:51:49.844116Z",
     "shell.execute_reply": "2021-12-04T03:51:49.843094Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.836110Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.845813Z",
     "iopub.status.busy": "2021-12-04T03:51:49.845568Z",
     "iopub.status.idle": "2021-12-04T03:51:49.859165Z",
     "shell.execute_reply": "2021-12-04T03:51:49.858204Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.845786Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We will tokenize data using counter vectorizer and TFIDF both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:49.860968Z",
     "iopub.status.busy": "2021-12-04T03:51:49.860385Z",
     "iopub.status.idle": "2021-12-04T03:51:51.423524Z",
     "shell.execute_reply": "2021-12-04T03:51:51.422642Z",
     "shell.execute_reply.started": "2021-12-04T03:51:49.860922Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), max_features=2500)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:51.425511Z",
     "iopub.status.busy": "2021-12-04T03:51:51.425107Z",
     "iopub.status.idle": "2021-12-04T03:51:52.953553Z",
     "shell.execute_reply": "2021-12-04T03:51:52.952774Z",
     "shell.execute_reply.started": "2021-12-04T03:51:51.425402Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=2500)\n",
    "X_train_tfidfvec = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidfvec = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualize data in 2D using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "embedded_data = TSNE(perplexity=100).fit_transform(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=embedded_data[:,0], y=embedded_data[:,1], hue=y_train, legend='full',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = TSNE(perplexity=200).fit_transform(X_train_vec, y_train)\n",
    "sns.scatterplot(x=embedded_data[:,0], y=embedded_data[:,1], hue=y_train, legend='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = TSNE(perplexity=300).fit_transform(X_train_vec, y_train)\n",
    "sns.scatterplot(x=embedded_data[:,0], y=embedded_data[:,1], hue=y_train, legend='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = TSNE(perplexity=80).fit_transform(X_train_vec, y_train)\n",
    "sns.scatterplot(x=embedded_data[:,0], y=embedded_data[:,1], hue=y_train, legend='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = TSNE(perplexity=50).fit_transform(X_train_vec, y_train)\n",
    "sns.scatterplot(x=embedded_data[:,0], y=embedded_data[:,1], hue=y_train, legend='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = TSNE(perplexity=400, n_iter=3000).fit_transform(X_train_vec, y_train)\n",
    "sns.scatterplot(x=embedded_data[:,0], y=embedded_data[:,1], hue=y_train, legend='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From TSNE result we can see that both classes are clearly separatable in 2D embedding of 2500 dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "1. Using count vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:52.964371Z",
     "iopub.status.busy": "2021-12-04T03:51:52.964140Z",
     "iopub.status.idle": "2021-12-04T03:51:55.742953Z",
     "shell.execute_reply": "2021-12-04T03:51:55.742269Z",
     "shell.execute_reply.started": "2021-12-04T03:51:52.964343Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC_model = RandomForestClassifier()\n",
    "RFC_model.fit(X_train_vec, y_train)\n",
    "RFC_model.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using TFIDF vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:55.744247Z",
     "iopub.status.busy": "2021-12-04T03:51:55.744012Z",
     "iopub.status.idle": "2021-12-04T03:51:58.590446Z",
     "shell.execute_reply": "2021-12-04T03:51:58.589632Z",
     "shell.execute_reply.started": "2021-12-04T03:51:55.744220Z"
    }
   },
   "outputs": [],
   "source": [
    "RFC_model_tfidf = RandomForestClassifier()\n",
    "RFC_model_tfidf.fit(X_train_tfidfvec, y_train)\n",
    "RFC_model_tfidf.score(X_test_tfidfvec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:58.591870Z",
     "iopub.status.busy": "2021-12-04T03:51:58.591631Z",
     "iopub.status.idle": "2021-12-04T03:51:59.119666Z",
     "shell.execute_reply": "2021-12-04T03:51:59.118727Z",
     "shell.execute_reply.started": "2021-12-04T03:51:58.591840Z"
    }
   },
   "outputs": [],
   "source": [
    "RFC_model_y_pred = RFC_model.predict(X_test_vec)\n",
    "RFC_model_tfidf_y_pred = RFC_model_tfidf.predict(X_test_tfidfvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:59.121177Z",
     "iopub.status.busy": "2021-12-04T03:51:59.120917Z",
     "iopub.status.idle": "2021-12-04T03:51:59.126037Z",
     "shell.execute_reply": "2021-12-04T03:51:59.125030Z",
     "shell.execute_reply.started": "2021-12-04T03:51:59.121142Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:59.127382Z",
     "iopub.status.busy": "2021-12-04T03:51:59.127163Z",
     "iopub.status.idle": "2021-12-04T03:51:59.400369Z",
     "shell.execute_reply": "2021-12-04T03:51:59.399455Z",
     "shell.execute_reply.started": "2021-12-04T03:51:59.127357Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"F1 Score for simple model : \", round(f1_score(y_test, RFC_model_y_pred)*100,2),\"%\")\n",
    "cm_1 = ConfusionMatrixDisplay(confusion_matrix(y_test, RFC_model_y_pred))\n",
    "cm_1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:59.402478Z",
     "iopub.status.busy": "2021-12-04T03:51:59.402061Z",
     "iopub.status.idle": "2021-12-04T03:51:59.670899Z",
     "shell.execute_reply": "2021-12-04T03:51:59.670084Z",
     "shell.execute_reply.started": "2021-12-04T03:51:59.402430Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"F1 Score for TFIDF model : \", round(f1_score(y_test, RFC_model_tfidf_y_pred)*100,2),\"%\")\n",
    "cm_2 = ConfusionMatrixDisplay(confusion_matrix(y_test, RFC_model_tfidf_y_pred))\n",
    "cm_2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:59.672532Z",
     "iopub.status.busy": "2021-12-04T03:51:59.672223Z",
     "iopub.status.idle": "2021-12-04T03:51:59.909745Z",
     "shell.execute_reply": "2021-12-04T03:51:59.908906Z",
     "shell.execute_reply.started": "2021-12-04T03:51:59.672490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, RFC_model_y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Random Forest')\n",
    "display.plot()\n",
    "print(\"roc_auc = \", round(roc_auc*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T03:51:59.911196Z",
     "iopub.status.busy": "2021-12-04T03:51:59.910942Z",
     "iopub.status.idle": "2021-12-04T03:52:00.158905Z",
     "shell.execute_reply": "2021-12-04T03:52:00.157243Z",
     "shell.execute_reply.started": "2021-12-04T03:51:59.911164Z"
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, RFC_model_tfidf_y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Random Forest (TFIDF)')\n",
    "display.plot()\n",
    "print(\"roc_auc = \", round(roc_auc*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "cross_val_score(RFC_model, X_train_vec, y_train, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(RFC_model, X_train_vec, y_train, cv=3, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RFC_model.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=vectorizer.get_feature_names())\n",
    "forest_importances.sort_values(inplace=True, ascending=False)\n",
    "forest_importances[:50].plot(kind='bar', figsize=(16,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(RFC_model_tfidf, X_train_tfidfvec, y_train, cv=3, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RFC_model_tfidf.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=tfidf_vectorizer.get_feature_names())\n",
    "forest_importances.sort_values(inplace=True, ascending=False)\n",
    "forest_importances[:50].plot(kind='bar', figsize=(16,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "1. We are getting really good accuracy and f1-score for both normal vectorizer and TFIDF vectorizer.  \n",
    "2. We have currently used only words **without any special characters**.  \n",
    "  \n",
    "Let's try to train Random Forest model on the data **with special characters** for TFIDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_special_characters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_special_characters[dataset_with_special_characters.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_special_characters.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_schar = dataset_with_special_characters['Query']\n",
    "y_schar = dataset_with_special_characters['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_schar, X_test_schar, y_train_schar, y_test_schar = train_test_split(X_schar, y_schar, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_schar = TfidfVectorizer(ngram_range=(1,3), max_features=2500)\n",
    "X_train_tfidfvec_schar = tfidf_vectorizer_schar.fit_transform(X_train_schar)\n",
    "X_test_tfidfvec_schar = tfidf_vectorizer_schar.transform(X_test_schar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_model_tfidf_schar = RandomForestClassifier()\n",
    "RFC_model_tfidf_schar.fit(X_train_tfidfvec_schar, y_train_schar)\n",
    "RFC_model_tfidf_schar.score(X_test_tfidfvec_schar, y_test_schar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_model_tfidf_y_pred_schar = RFC_model_tfidf_schar.predict(X_test_tfidfvec_schar)\n",
    "print(\"F1 Score for simple model : \", round(f1_score(y_test_schar, RFC_model_tfidf_y_pred_schar)*100,2),\"%\")\n",
    "cm_1 = ConfusionMatrixDisplay(confusion_matrix(y_test_schar, RFC_model_tfidf_y_pred_schar))\n",
    "cm_1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Here we can see that model is making more errors for type-1 while special characters are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(estimator=RFC_model_tfidf_schar,param_grid={'n_estimators': [10, 100, 500], 'oob_score': (True, False)}, n_jobs=-1, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_tfidfvec_schar, y_train_schar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "cv_results.sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RFC_model_tfidf_schar_bestCV = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC_model_tfidf_schar_bestCV.fit(X_train_tfidfvec_schar, y_train_schar)\n",
    "RFC_model_tfidf_schar_bestCV.score(X_test_tfidfvec_schar, y_test_schar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_model_tfidf_y_pred_schar = RFC_model_tfidf_schar_bestCV.predict(X_test_tfidfvec_schar)\n",
    "print(\"F1 Score for simple model : \", round(f1_score(y_test_schar, RFC_model_tfidf_y_pred_schar)*100,2),\"%\")\n",
    "cm_1 = ConfusionMatrixDisplay(confusion_matrix(y_test_schar, RFC_model_tfidf_y_pred_schar))\n",
    "cm_1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even After doing hyperparameter tuning, we are not getting better score with special characters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "1. We get better results by removing special characters.\n",
    "2. We get almost same result for normal count vectorizer and TFIDF vectorizer.\n",
    "3. Random Forest model with default parameters works the best."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50b7b7039633aa29578af233b219e5c1be76f227168bf566501c2bcf30befc26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
